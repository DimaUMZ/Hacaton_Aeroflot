{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471b164e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_image_size(image_path):\n",
    "    \"\"\"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return (0.5, 0.5, 0.8, 0.8)  # fallback values\n",
    "    \n",
    "    height, width = img.shape[:2]\n",
    "    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∑–∞–Ω–∏–º–∞–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é —á–∞—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    bbox_width = 0.7 * width\n",
    "    bbox_height = 0.7 * height\n",
    "    x_center = width / 2\n",
    "    y_center = height / 2\n",
    "    \n",
    "    # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è YOLO\n",
    "    x_center_norm = x_center / width\n",
    "    y_center_norm = y_center / height\n",
    "    width_norm = bbox_width / width\n",
    "    height_norm = bbox_height / height\n",
    "    \n",
    "    return (x_center_norm, y_center_norm, width_norm, height_norm)\n",
    "\n",
    "def copy_images_with_annotations(src_dir, output_dir, class_idx, split, images, is_single_tool=True):\n",
    "    \"\"\"–ö–æ–ø–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\"\"\"\n",
    "    for img_name in images:\n",
    "        try:\n",
    "            # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "            src_img = os.path.join(src_dir, img_name)\n",
    "            dst_img = os.path.join(output_dir, 'images', split, img_name)\n",
    "            shutil.copy2(src_img, dst_img)\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–µ–º YOLO –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é\n",
    "            txt_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "            \n",
    "            if is_single_tool:\n",
    "                # –î–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å–æ–∑–¥–∞–µ–º bbox –ø–æ —Ü–µ–Ω—Ç—Ä—É\n",
    "                bbox_coords = analyze_image_size(src_img)\n",
    "                with open(os.path.join(output_dir, 'labels', split, txt_name), 'w') as f:\n",
    "                    f.write(f\"{class_idx} {bbox_coords[0]} {bbox_coords[1]} {bbox_coords[2]} {bbox_coords[3]}\\n\")\n",
    "            else:\n",
    "                # –î–ª—è –≥—Ä—É–ø–ø–æ–≤—ã—Ö –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å –ª–∏–Ω–µ–π–∫–æ–π - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "                annotation_path = os.path.join(src_dir, txt_name)\n",
    "                if os.path.exists(annotation_path):\n",
    "                    # –ö–æ–ø–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é\n",
    "                    dst_txt = os.path.join(output_dir, 'labels', split, txt_name)\n",
    "                    shutil.copy2(annotation_path, dst_txt)\n",
    "                else:\n",
    "                    # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é (—Ç—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏)\n",
    "                    dst_txt = os.path.join(output_dir, 'labels', split, txt_name)\n",
    "                    open(dst_txt, 'w').close()\n",
    "                    print(f\"  ‚ö† –°–æ–∑–¥–∞–Ω–∞ –ø—É—Å—Ç–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –¥–ª—è {img_name} (—Ç—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏)\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {img_name}: {e}\")\n",
    "\n",
    "def process_single_tools(dataset_path, output_dir, classes):\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–∞–ø–∫–∏ —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏\"\"\"\n",
    "    print(\"\\n=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ ===\")\n",
    "    \n",
    "    total_images = 0\n",
    "    for class_idx, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        \n",
    "        if not os.path.exists(class_path):\n",
    "            print(f\"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ø–∞–ø–∫–∞ {class_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!\")\n",
    "            continue\n",
    "            \n",
    "        images = [f for f in os.listdir(class_path) \n",
    "                 if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "        \n",
    "        print(f\"{class_name}: {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "        \n",
    "        if len(images) == 0:\n",
    "            print(f\"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –≤ –ø–∞–ø–∫–µ {class_name} –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π!\")\n",
    "            continue\n",
    "        \n",
    "        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val\n",
    "        if len(images) == 1:\n",
    "            train_imgs = images\n",
    "            val_imgs = []\n",
    "        else:\n",
    "            train_imgs, val_imgs = train_test_split(images, test_size=0.2, random_state=42, shuffle=True)\n",
    "        \n",
    "        print(f\"  Train: {len(train_imgs)}, Val: {len(val_imgs)}\")\n",
    "        total_images += len(images)\n",
    "        \n",
    "        # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "        copy_images_with_annotations(class_path, output_dir, class_idx, 'train', train_imgs, is_single_tool=True)\n",
    "        copy_images_with_annotations(class_path, output_dir, class_idx, 'val', val_imgs, is_single_tool=True)\n",
    "    \n",
    "    print(f\"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤: {total_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "    return total_images\n",
    "\n",
    "def process_tools_with_ruler(dataset_path, output_dir, ruler_folder, classes):\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Å –ª–∏–Ω–µ–π–∫–æ–π\"\"\"\n",
    "    print(f\"\\n=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å –ª–∏–Ω–µ–π–∫–æ–π: {ruler_folder} ===\")\n",
    "    \n",
    "    ruler_path = os.path.join(dataset_path, ruler_folder)\n",
    "    \n",
    "    if not os.path.exists(ruler_path):\n",
    "        print(f\"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ø–∞–ø–∫–∞ {ruler_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!\")\n",
    "        return 0\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∏–º–µ–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∫ –∫–ª–∞—Å—Å–∞–º\n",
    "    class_mapping = {}\n",
    "    for class_idx, class_name in enumerate(classes):\n",
    "        class_mapping[class_name] = class_idx\n",
    "    \n",
    "    total_images = 0\n",
    "    images = [f for f in os.listdir(ruler_path) \n",
    "             if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "    \n",
    "    print(f\"–ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ª–∏–Ω–µ–π–∫–æ–π: {len(images)}\")\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        print(\"‚ö† –í –ø–∞–ø–∫–µ —Å –ª–∏–Ω–µ–π–∫–æ–π –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π!\")\n",
    "        return 0\n",
    "    \n",
    "    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val\n",
    "    if len(images) == 1:\n",
    "        train_imgs = images\n",
    "        val_imgs = []\n",
    "    else:\n",
    "        train_imgs, val_imgs = train_test_split(images, test_size=0.2, random_state=42, shuffle=True)\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞\n",
    "    for split, imgs in [('train', train_imgs), ('val', val_imgs)]:\n",
    "        for img_name in imgs:\n",
    "            try:\n",
    "                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞\n",
    "                tool_class = None\n",
    "                for class_name in classes:\n",
    "                    if class_name.lower() in img_name.lower():\n",
    "                        tool_class = class_mapping[class_name]\n",
    "                        break\n",
    "                \n",
    "                if tool_class is None:\n",
    "                    print(f\"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–ª–∞—Å—Å –¥–ª—è {img_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "                src_img = os.path.join(ruler_path, img_name)\n",
    "                dst_img = os.path.join(output_dir, 'images', split, img_name)\n",
    "                shutil.copy2(src_img, dst_img)\n",
    "                \n",
    "                # –°–æ–∑–¥–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é\n",
    "                txt_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "                copy_images_with_annotations(ruler_path, output_dir, tool_class, split, [img_name], is_single_tool=False)\n",
    "                \n",
    "                total_images += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {img_name}: {e}\")\n",
    "    \n",
    "    print(f\"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å –ª–∏–Ω–µ–π–∫–æ–π: {total_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "    return total_images\n",
    "\n",
    "def process_group_photos(dataset_path, output_dir, group_folders, classes):\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥—Ä—É–ø–ø–æ–≤—ã–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤\"\"\"\n",
    "    print(\"\\n=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø–æ–≤—ã—Ö —Ñ–æ—Ç–æ ===\")\n",
    "    \n",
    "    total_images = 0\n",
    "    for group_folder in group_folders:\n",
    "        group_path = os.path.join(dataset_path, group_folder)\n",
    "        \n",
    "        if not os.path.exists(group_path):\n",
    "            print(f\"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ø–∞–ø–∫–∞ {group_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!\")\n",
    "            continue\n",
    "            \n",
    "        images = [f for f in os.listdir(group_path) \n",
    "                 if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "        \n",
    "        print(f\"{group_folder}: {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "        \n",
    "        if len(images) == 0:\n",
    "            print(f\"‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –≤ –ø–∞–ø–∫–µ {group_folder} –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π!\")\n",
    "            continue\n",
    "        \n",
    "        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val\n",
    "        if len(images) == 1:\n",
    "            train_imgs = images\n",
    "            val_imgs = []\n",
    "        else:\n",
    "            train_imgs, val_imgs = train_test_split(images, test_size=0.2, random_state=42, shuffle=True)\n",
    "        \n",
    "        # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "        for split, imgs in [('train', train_imgs), ('val', val_imgs)]:\n",
    "            for img_name in imgs:\n",
    "                try:\n",
    "                    # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "                    src_img = os.path.join(group_path, img_name)\n",
    "                    dst_img = os.path.join(output_dir, 'images', split, img_name)\n",
    "                    shutil.copy2(src_img, dst_img)\n",
    "                    \n",
    "                    # –ö–æ–ø–∏—Ä—É–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é (–µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)\n",
    "                    txt_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "                    annotation_path = os.path.join(group_path, txt_name)\n",
    "                    \n",
    "                    if os.path.exists(annotation_path):\n",
    "                        dst_txt = os.path.join(output_dir, 'labels', split, txt_name)\n",
    "                        shutil.copy2(annotation_path, dst_txt)\n",
    "                        print(f\"  ‚úì –î–æ–±–∞–≤–ª–µ–Ω–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –¥–ª—è {img_name}\")\n",
    "                    else:\n",
    "                        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é\n",
    "                        dst_txt = os.path.join(output_dir, 'labels', split, txt_name)\n",
    "                        open(dst_txt, 'w').close()\n",
    "                        print(f\"  ‚ö† –°–æ–∑–¥–∞–Ω–∞ –ø—É—Å—Ç–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –¥–ª—è {img_name} (—Ç—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏)\")\n",
    "                    \n",
    "                    total_images += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥—Ä—É–ø–ø–æ–≤–æ–≥–æ —Ñ–æ—Ç–æ {img_name}: {e}\")\n",
    "    \n",
    "    print(f\"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≥—Ä—É–ø–ø–æ–≤—ã—Ö —Ñ–æ—Ç–æ: {total_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "    return total_images\n",
    "\n",
    "def check_dataset_stats(output_dir):\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\"\"\"\n",
    "    print(\"\\n=== –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ ===\")\n",
    "    \n",
    "    train_images = len(os.listdir(os.path.join(output_dir, 'images', 'train')))\n",
    "    val_images = len(os.listdir(os.path.join(output_dir, 'images', 'val')))\n",
    "    train_labels = len(os.listdir(os.path.join(output_dir, 'labels', 'train')))\n",
    "    val_labels = len(os.listdir(os.path.join(output_dir, 'labels', 'val')))\n",
    "    \n",
    "    print(f\"‚úì –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {train_images}\")\n",
    "    print(f\"‚úì –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {val_images}\")\n",
    "    print(f\"‚úì –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {train_labels}\")\n",
    "    print(f\"‚úì –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {val_labels}\")\n",
    "    print(f\"‚úì –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {train_images + val_images}\")\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö\n",
    "    total_objects = 0\n",
    "    for split in ['train', 'val']:\n",
    "        labels_dir = os.path.join(output_dir, 'labels', split)\n",
    "        for label_file in os.listdir(labels_dir):\n",
    "            if label_file.endswith('.txt'):\n",
    "                with open(os.path.join(labels_dir, label_file), 'r') as f:\n",
    "                    objects = len(f.readlines())\n",
    "                    total_objects += objects\n",
    "    \n",
    "    print(f\"‚úì –í—Å–µ–≥–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö: {total_objects}\")\n",
    "\n",
    "def prepare_yolo_dataset(dataset_path, output_dir='yolo_dataset'):\n",
    "    \"\"\"\n",
    "    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLO –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "    \"\"\"\n",
    "    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–∞–ø–∫–∏\n",
    "    all_folders = sorted([d for d in os.listdir(dataset_path) \n",
    "                         if os.path.isdir(os.path.join(dataset_path, d))])\n",
    "    \n",
    "    print(f\"–ù–∞–π–¥–µ–Ω–æ –ø–∞–ø–æ–∫: {len(all_folders)}\")\n",
    "    print(\"–í—Å–µ –ø–∞–ø–∫–∏:\", all_folders)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "    for split in ['train', 'val']:\n",
    "        os.makedirs(os.path.join(output_dir, 'images', split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, 'labels', split), exist_ok=True)\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –ø–∞–ø–æ–∫\n",
    "    tools_folders = [folder for folder in all_folders \n",
    "                    if '–≥—Ä—É–ø–ø–æ–≤—ã–µ' not in folder.lower() \n",
    "                    and '–ª–∏–Ω–µ–π–∫' not in folder.lower()\n",
    "                    and 'group' not in folder.lower()\n",
    "                    and 'ruler' not in folder.lower()]\n",
    "    \n",
    "    ruler_folders = [folder for folder in all_folders \n",
    "                    if '–ª–∏–Ω–µ–π–∫' in folder.lower() \n",
    "                    or 'ruler' in folder.lower()]\n",
    "    \n",
    "    group_folders = [folder for folder in all_folders \n",
    "                    if '–≥—Ä—É–ø–ø–æ–≤—ã–µ' in folder.lower() \n",
    "                    or 'group' in folder.lower()]\n",
    "    \n",
    "    print(f\"‚úì –ü–∞–ø–∫–∏ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏: {len(tools_folders)}\")\n",
    "    print(\"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:\", tools_folders)\n",
    "    print(f\"‚úì –ü–∞–ø–∫–∏ —Å –ª–∏–Ω–µ–π–∫–æ–π: {len(ruler_folders)}\")\n",
    "    print(\"–õ–∏–Ω–µ–π–∫–∞:\", ruler_folders)\n",
    "    print(f\"‚úì –ì—Ä—É–ø–ø–æ–≤—ã–µ –ø–∞–ø–∫–∏: {len(group_folders)}\")\n",
    "    print(\"–ì—Ä—É–ø–ø–æ–≤—ã–µ:\", group_folders)\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–ª–∞—Å—Å—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤\n",
    "    with open(os.path.join(output_dir, 'classes.txt'), 'w') as f:\n",
    "        for i, class_name in enumerate(tools_folders):\n",
    "            f.write(f\"{class_name}\\n\")\n",
    "    \n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö\n",
    "    single_count = process_single_tools(dataset_path, output_dir, tools_folders)\n",
    "    ruler_count = 0\n",
    "    for ruler_folder in ruler_folders:\n",
    "        ruler_count += process_tools_with_ruler(dataset_path, output_dir, ruler_folder, tools_folders)\n",
    "    \n",
    "    group_count = process_group_photos(dataset_path, output_dir, group_folders, tools_folders)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "    check_dataset_stats(output_dir)\n",
    "    \n",
    "    print(f\"\\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n",
    "    print(f\"  –û–¥–∏–Ω–æ—á–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: {single_count}\")\n",
    "    print(f\"  –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Å –ª–∏–Ω–µ–π–∫–æ–π: {ruler_count}\")\n",
    "    print(f\"  –ì—Ä—É–ø–ø–æ–≤—ã–µ —Ñ–æ—Ç–æ: {group_count}\")\n",
    "    print(f\"  –í—Å–µ–≥–æ: {single_count + ruler_count + group_count}\")\n",
    "    \n",
    "    return tools_folders\n",
    "\n",
    "def create_dataset_config(output_dir='yolo_dataset', output_yaml='dataset.yaml'):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ YOLO\n",
    "    \"\"\"\n",
    "    # –ß–∏—Ç–∞–µ–º –∫–ª–∞—Å—Å—ã –∏–∑ —Ñ–∞–π–ª–∞\n",
    "    classes_path = os.path.join(output_dir, 'classes.txt')\n",
    "    if not os.path.exists(classes_path):\n",
    "        print(f\"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª {classes_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "        return None, None\n",
    "    \n",
    "    with open(classes_path, 'r') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π\n",
    "    config = {\n",
    "        'path': os.path.abspath(output_dir),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'nc': len(classes),\n",
    "        'names': {i: class_name for i, class_name in enumerate(classes)}\n",
    "    }\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ YAML —Ñ–∞–π–ª\n",
    "    with open(output_yaml, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "    \n",
    "    print(f\"‚úì –°–æ–∑–¥–∞–Ω –∫–æ–Ω—Ñ–∏–≥ —Ñ–∞–π–ª: {output_yaml}\")\n",
    "    print(f\"‚úì –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: {os.path.abspath(output_dir)}\")\n",
    "    print(f\"‚úì –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(classes)}\")\n",
    "    print(f\"‚úì –ö–ª–∞—Å—Å—ã: {classes}\")\n",
    "    return config, classes\n",
    "\n",
    "def train_yolo_model(output_dir='yolo_dataset', model_size='s', epochs=100):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ YOLOv8 —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "    \"\"\"\n",
    "    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª\n",
    "    config, classes = create_dataset_config(output_dir)\n",
    "    \n",
    "    if config is None:\n",
    "        raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "    print(f\"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ YOLOv8{model_size}.pt...\")\n",
    "    model = YOLO(f'yolov8{model_size}.pt')\n",
    "    \n",
    "    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n",
    "    train_params = {\n",
    "        'data': 'dataset.yaml',\n",
    "        'epochs': epochs,\n",
    "        'imgsz': 640,\n",
    "        'batch': 16,\n",
    "        'name': f'yolov8{model_size}_tools_detection',\n",
    "        'patience': 20,\n",
    "        'optimizer': 'AdamW',\n",
    "        'lr0': 0.001,\n",
    "        'lrf': 0.01,\n",
    "        'momentum': 0.937,\n",
    "        'weight_decay': 0.0005,\n",
    "        'augment': True,\n",
    "        'hsv_h': 0.015,\n",
    "        'hsv_s': 0.7,\n",
    "        'hsv_v': 0.4,\n",
    "        'degrees': 45.0,\n",
    "        'translate': 0.1,\n",
    "        'scale': 0.5,\n",
    "        'shear': 0.0,\n",
    "        'perspective': 0.0,\n",
    "        'flipud': 0.0,\n",
    "        'fliplr': 0.5,\n",
    "        'mosaic': 1.0,\n",
    "        'mixup': 0.1,\n",
    "        'copy_paste': 0.1,\n",
    "        'erasing': 0.4,\n",
    "        'dropout': 0.1,\n",
    "        'val': True,\n",
    "        'save': True,\n",
    "        'save_period': 10,\n",
    "        'device': 'cpu',  # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ 'cuda' –∏–ª–∏ 0 –¥–ª—è GPU\n",
    "        'workers': 8,\n",
    "        'single_cls': False,\n",
    "        'verbose': True,\n",
    "        'exist_ok': True\n",
    "    }\n",
    "    \n",
    "    print(\"üéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "    print(f\"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è: {epochs} —ç–ø–æ—Ö, —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: 16\")\n",
    "    print(f\"üìÅ –î–∞–Ω–Ω—ã–µ: {config['path']}\")\n",
    "    \n",
    "    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "    results = model.train(**train_params)\n",
    "    \n",
    "    print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
    "    return model, results\n",
    "\n",
    "def export_model_to_onnx(model, output_path='yolov8_tools.onnx'):\n",
    "    \"\"\"\n",
    "    –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç ONNX —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π\n",
    "    \"\"\"\n",
    "    print(\"üì§ –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ –≤ ONNX...\")\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "    model_path = model.trainer.best if hasattr(model.trainer, 'best') else 'runs/detect/yolov8s_tools_detection/weights/best.pt'\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞\n",
    "        best_model = YOLO(model_path)\n",
    "        best_model.export(format='onnx', imgsz=640, simplify=True, dynamic=True, opset=12)\n",
    "        print(f\"‚úÖ –ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤: {output_path}\")\n",
    "    else:\n",
    "        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å\n",
    "        model.export(format='onnx', imgsz=640, simplify=True, dynamic=True, opset=12)\n",
    "        print(f\"‚úÖ –ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤: {output_path}\")\n",
    "\n",
    "def evaluate_model(model, data_path='yolo_dataset'):\n",
    "    \"\"\"\n",
    "    –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    \"\"\"\n",
    "    print(\"üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...\")\n",
    "    \n",
    "    try:\n",
    "        metrics = model.val(data=os.path.join(data_path, 'dataset.yaml'), split='val')\n",
    "        \n",
    "        print(\"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:\")\n",
    "        print(f\"  mAP50: {metrics.box.map50:.4f}\")\n",
    "        print(f\"  mAP50-95: {metrics.box.map:.4f}\")\n",
    "        print(f\"  Precision: {metrics.box.mp:.4f}\")\n",
    "        print(f\"  Recall: {metrics.box.mr:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_on_image(model, image_path, conf_threshold=0.25, iou_threshold=0.45):\n",
    "    \"\"\"\n",
    "    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π\n",
    "    \"\"\"\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "    results = model(image_path, conf=conf_threshold, iou=iou_threshold, augment=False)\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "    for i, result in enumerate(results):\n",
    "        # –†–∏—Å—É–µ–º bounding boxes —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π\n",
    "        img = result.plot(line_width=2, font_size=1.0, conf=True, labels=True)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –æ–∫–Ω–æ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º\n",
    "        height, width = img.shape[:2]\n",
    "        max_display_size = 1200\n",
    "        if max(height, width) > max_display_size:\n",
    "            scale = max_display_size / max(height, width)\n",
    "            new_width = int(width * scale)\n",
    "            new_height = int(height * scale)\n",
    "            img = cv2.resize(img, (new_width, new_height))\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "        cv2.imshow('–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã - –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ', img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–∞—Ö\n",
    "        print(f\"\\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i+1}:\")\n",
    "        if len(result.boxes) > 0:\n",
    "            for j, box in enumerate(result.boxes):\n",
    "                class_id = int(box.cls[0])\n",
    "                confidence = float(box.conf[0])\n",
    "                bbox = box.xyxy[0].cpu().numpy()\n",
    "                class_name = model.names[class_id]\n",
    "                print(f\"  –û–±—ä–µ–∫—Ç {j+1}: {class_name} \"\n",
    "                      f\"(—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f}) \"\n",
    "                      f\"BBox: {bbox.astype(int)}\")\n",
    "        else:\n",
    "            print(\"  –û–±—ä–µ–∫—Ç—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def predict_on_folder(model, folder_path, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –≤ –ø–∞–ø–∫–µ\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "    images = [f for f in os.listdir(folder_path) if f.lower().endswith(image_extensions)]\n",
    "    \n",
    "    print(f\"üîç –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø–∞–ø–∫–µ {folder_path}\")\n",
    "    \n",
    "    for i, img_name in enumerate(images):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        print(f\"\\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {i+1}/{len(images)}: {img_name}\")\n",
    "        predict_on_image(model, img_path, conf_threshold)\n",
    "\n",
    "def main():\n",
    "    # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º\n",
    "    dataset_path = '/data/vscode/HacatonAeroflot/Aeroflot-project/datasets/raw'\n",
    "    output_dir = '/data/vscode/HacatonAeroflot/Aeroflot-project/yolo_dataset'\n",
    "    \n",
    "    try:\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø—É—Ç–∏\n",
    "        if not os.path.exists(dataset_path):\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞: –ø—É—Ç—å {dataset_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"üõ†Ô∏è  –°–ò–°–¢–ï–ú–ê –û–ë–ù–ê–†–£–ñ–ï–ù–ò–Ø –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í (–£–õ–£–ß–®–ï–ù–ù–ê–Ø)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "        print(\"\\nüìÅ –®–ê–ì 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...\")\n",
    "        classes = prepare_yolo_dataset(dataset_path, output_dir)\n",
    "        \n",
    "        # –®–∞–≥ 2: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "        print(\"\\n‚öôÔ∏è  –®–ê–ì 2: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...\")\n",
    "        config, classes = create_dataset_config(output_dir)\n",
    "        \n",
    "        if config is None:\n",
    "            raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞\")\n",
    "        \n",
    "        # –®–∞–≥ 3: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "        print(\"\\nüéì –®–ê–ì 3: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "        model, results = train_yolo_model(output_dir, model_size='s', epochs=100)\n",
    "        \n",
    "        # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "        print(\"\\nüíæ –®–ê–ì 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "        model.save('best_tools_detection.pt')\n",
    "        print(\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'best_tools_detection.pt'\")\n",
    "        \n",
    "        # –®–∞–≥ 5: –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX\n",
    "        print(\"\\nüì§ –®–ê–ì 5: –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX...\")\n",
    "        export_model_to_onnx(model)\n",
    "        \n",
    "        # –®–∞–≥ 6: –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "        print(\"\\nüìä –®–ê–ì 6: –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...\")\n",
    "        metrics = evaluate_model(model, output_dir)\n",
    "        \n",
    "        # –®–∞–≥ 7: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "        print(\"\\nüß™ –®–ê–ì 7: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...\")\n",
    "        \n",
    "        test_types = ['val']  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å 'train' –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ—Å—Ç–æ–≤\n",
    "        \n",
    "        for split in test_types:\n",
    "            test_dir = os.path.join(output_dir, 'images', split)\n",
    "            if os.path.exists(test_dir):\n",
    "                test_images = os.listdir(test_dir)\n",
    "                if test_images:\n",
    "                    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö\n",
    "                    for i, test_img in enumerate(test_images[:3]):  # –ü–µ—Ä–≤—ã–µ 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "                        test_image_path = os.path.join(test_dir, test_img)\n",
    "                        print(f\"\\nüîç –¢–µ—Å—Ç {i+1}: {test_img}\")\n",
    "                        predict_on_image(model, test_image_path)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ –í–°–ï –≠–¢–ê–ü–´ –ó–ê–í–ï–†–®–ï–ù–´ –£–°–ü–ï–®–ù–û!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
